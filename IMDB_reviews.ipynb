{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from plotly->catboost) (8.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk) (2023.5.5)\nRequirement already satisfied: numpy>=1.17.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from gensim) (1.23.1)\n"
                }
            ],
            "source": "!pip install catboost | tail -n 1\n!pip install scikit-plot | tail -n 1\n!pip install nltk | tail -n 1\n!pip install gensim | tail -n 1"
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np\n\nimport re, math, os\n\nimport string\n\nfrom collections import Counter, defaultdict\n\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport catboost as ctb\n#import lightgbm as lgb\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom scikitplot.metrics import plot_confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline"
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": "with open(\"imdb.zip\", \"wb\") as outfile:\n    # Copy the BytesIO stream to the output file\n    outfile.write(data.read())"
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": "from zipfile import ZipFile\n\nwith ZipFile('imdb.zip', 'r') as zipObj:\n    zipObj.extractall()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Function for Preprocessing"
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package stopwords to /home/wsuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/wsuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
                },
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "execution_count": 59,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')"
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": "from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nstop_words = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()\n\n\nclass Preprocessing():\n    \n    def lowercase(self, text: pd.core.series.Series):\n        return text.str.lower()\n\n    def remove_digits(self, text: pd.core.series.Series):\n        return text.str.replace('\\d+', '', regex = True)\n\n    def remove_punctuation(self, text: pd.core.series.Series):\n        return text.str.replace('[{}]'.format(string.punctuation), '', regex = True)\n\n    def remove_whitespaces(self, text: pd.core.series.Series):\n        text = text.replace('\\s+',' ', regex=True)\n        return text[text != ' ']\n\n    def remove_newlines(self, text: pd.core.series.Series):\n        return text.str.replace(r'\\n', '', regex = True)\n\n    def remove_stopwords(self, text: pd.core.series.Series):\n        return text.apply(lambda x: ' '.join([w for w in x.split() if not w in stop_words]))\n\n    def lemmatizing(self, text: pd.core.series.Series):\n        return text.apply(lambda x: ' '.join([lemmatizer.lemmatize(w) for w in x.split()]))\n\n    def mapping_english(self, text: pd.core.series.Series):\n        return text.apply(lambda x: True if re.search(u'[\\x80-\\U0010ffff]', x) == None else False)\n\n    def check_nans(self, text: pd.core.series.Series):\n        if text.isnull().sum() == 0:\n            print('No NaNs')\n        else:\n            print('Dropping NaNs')\n            text.dropna()\n        return text\n    \n    def transform(self, text, \n                 lowercase = True, remove_dig = True, remove_punc = True, \n                 whitespaces = True, newlines = True, stopwords = True, lemmatize = True):\n        if lowercase:\n            text = self.lowercase(text)\n        if remove_dig:\n            text = self.remove_digits(text)\n        if remove_punc:\n            text = self.remove_punctuation(text)\n        if whitespaces:\n            text = self.remove_whitespaces(text)\n        if newlines:\n            text = self.remove_newlines(text)\n        if stopwords:\n            text = self.remove_stopwords(text)\n        if lemmatize:\n            text = self.lemmatizing(text)\n        return text   "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Creating training set"
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": "imdb_dir = './aclImdb'\ntrain_dir = os.path.join(imdb_dir, 'train')\n\nlabels = []\ntexts = []\n\nfor label_type in ['neg', 'pos']:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)"
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17797</th>\n      <td>1</td>\n      <td>This was Barbra Streisand's first television s...</td>\n    </tr>\n    <tr>\n      <th>6458</th>\n      <td>0</td>\n      <td>I think Jason Lee has huge potential, but this...</td>\n    </tr>\n    <tr>\n      <th>14478</th>\n      <td>1</td>\n      <td>Having developed a critical eye for film, and ...</td>\n    </tr>\n    <tr>\n      <th>14825</th>\n      <td>1</td>\n      <td>This picks up about a year after the events in...</td>\n    </tr>\n    <tr>\n      <th>7523</th>\n      <td>0</td>\n      <td>This movie could have been very good, but come...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>0</td>\n      <td>Even the first 10 minutes of this movie were h...</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>0</td>\n      <td>I confess--Emma, in my opinion, is the single ...</td>\n    </tr>\n    <tr>\n      <th>5848</th>\n      <td>0</td>\n      <td>I rented this horrible movie. The worst think ...</td>\n    </tr>\n    <tr>\n      <th>2439</th>\n      <td>0</td>\n      <td>I wouldn't normally write a comment on-line, b...</td>\n    </tr>\n    <tr>\n      <th>23572</th>\n      <td>1</td>\n      <td>Whether it's three guys in their tighty-whitey...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows \u00d7 2 columns</p>\n</div>",
                        "text/plain": "       labels                                              texts\n17797       1  This was Barbra Streisand's first television s...\n6458        0  I think Jason Lee has huge potential, but this...\n14478       1  Having developed a critical eye for film, and ...\n14825       1  This picks up about a year after the events in...\n7523        0  This movie could have been very good, but come...\n...       ...                                                ...\n57          0  Even the first 10 minutes of this movie were h...\n578         0  I confess--Emma, in my opinion, is the single ...\n5848        0  I rented this horrible movie. The worst think ...\n2439        0  I wouldn't normally write a comment on-line, b...\n23572       1  Whether it's three guys in their tighty-whitey...\n\n[25000 rows x 2 columns]"
                    },
                    "execution_count": 62,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# create and load df from lists 'labels' and 'texts'\ndf_train = pd.DataFrame(columns=['labels', 'texts'])\ndf_train['labels']=labels\ndf_train['texts']=texts\n#mixing\ndf_train = df_train.sample(frac = 1.0, random_state=33)\ndf_train"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Creating test set"
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": "imdb_dir = './aclImdb'\ntest_dir = os.path.join(imdb_dir, 'test')\n\nlabels_test = []\ntexts_test = []\n\nfor label_type in ['neg', 'pos']:\n    dir_name = os.path.join(test_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts_test.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels_test.append(0)\n            else:\n                labels_test.append(1)"
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [],
            "source": "df_test = pd.DataFrame(columns=['labels', 'texts'])\ndf_test['labels']=labels_test\ndf_test['texts']=texts_test\n#mixing\ndf_test = df_test.sample(frac = 1.0, random_state=33)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Preprocessing"
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "No NaNs\nNo NaNs\n"
                }
            ],
            "source": "clean_x_train = Preprocessing().check_nans(df_train['texts'])\nclean_x_test = Preprocessing().check_nans(df_test['texts'])\n\ntransformed_x_train= Preprocessing().transform(clean_x_train)\ntransformed_x_test = Preprocessing().transform(clean_x_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": "df_train['preprocessed_texts'] = transformed_x_train\ndf_test['preprocessed_texts'] = transformed_x_test"
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>texts</th>\n      <th>preprocessed_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17797</th>\n      <td>1</td>\n      <td>This was Barbra Streisand's first television s...</td>\n      <td>barbra streisand first television special must...</td>\n    </tr>\n    <tr>\n      <th>6458</th>\n      <td>0</td>\n      <td>I think Jason Lee has huge potential, but this...</td>\n      <td>think jason lee huge potential wrong vehicle a...</td>\n    </tr>\n    <tr>\n      <th>14478</th>\n      <td>1</td>\n      <td>Having developed a critical eye for film, and ...</td>\n      <td>developed critical eye film love good cinema w...</td>\n    </tr>\n    <tr>\n      <th>14825</th>\n      <td>1</td>\n      <td>This picks up about a year after the events in...</td>\n      <td>pick year event basic instinct catherine trame...</td>\n    </tr>\n    <tr>\n      <th>7523</th>\n      <td>0</td>\n      <td>This movie could have been very good, but come...</td>\n      <td>movie could good come way short cheesy special...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>0</td>\n      <td>Even the first 10 minutes of this movie were h...</td>\n      <td>even first minute movie horrific hard believe ...</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>0</td>\n      <td>I confess--Emma, in my opinion, is the single ...</td>\n      <td>confessemma opinion single greatest novel ever...</td>\n    </tr>\n    <tr>\n      <th>5848</th>\n      <td>0</td>\n      <td>I rented this horrible movie. The worst think ...</td>\n      <td>rented horrible movie worst think ever seen be...</td>\n    </tr>\n    <tr>\n      <th>2439</th>\n      <td>0</td>\n      <td>I wouldn't normally write a comment on-line, b...</td>\n      <td>wouldnt normally write comment online worst mo...</td>\n    </tr>\n    <tr>\n      <th>23572</th>\n      <td>1</td>\n      <td>Whether it's three guys in their tighty-whitey...</td>\n      <td>whether three guy tightywhiteys rapping dude b...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows \u00d7 3 columns</p>\n</div>",
                        "text/plain": "       labels                                              texts  \\\n17797       1  This was Barbra Streisand's first television s...   \n6458        0  I think Jason Lee has huge potential, but this...   \n14478       1  Having developed a critical eye for film, and ...   \n14825       1  This picks up about a year after the events in...   \n7523        0  This movie could have been very good, but come...   \n...       ...                                                ...   \n57          0  Even the first 10 minutes of this movie were h...   \n578         0  I confess--Emma, in my opinion, is the single ...   \n5848        0  I rented this horrible movie. The worst think ...   \n2439        0  I wouldn't normally write a comment on-line, b...   \n23572       1  Whether it's three guys in their tighty-whitey...   \n\n                                      preprocessed_texts  \n17797  barbra streisand first television special must...  \n6458   think jason lee huge potential wrong vehicle a...  \n14478  developed critical eye film love good cinema w...  \n14825  pick year event basic instinct catherine trame...  \n7523   movie could good come way short cheesy special...  \n...                                                  ...  \n57     even first minute movie horrific hard believe ...  \n578    confessemma opinion single greatest novel ever...  \n5848   rented horrible movie worst think ever seen be...  \n2439   wouldnt normally write comment online worst mo...  \n23572  whether three guy tightywhiteys rapping dude b...  \n\n[25000 rows x 3 columns]"
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_train"
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "------------BEFORE-------------\nI just thought it was excellent and I still do. I'm grateful we're still able to see different stuff from what Hollywood almost floods us with. Saving Grace is smart and enjoyable - those who feel offended by the marijuana thing better go see the America's bride sort of movie.<br /><br />Saving Grace also shows that a funny movie doesn't have to be stupid. I was laughing my ass off during most of it but also pondering questions about what was the female lead character supposed to do to pay her deceased husband's debts.<br /><br />In a nutshell - a witty storyline with typical English humour and good acting and directing. You couldn't ask for more.<br /><br />7/10.\n------------AFTER-------------\nthought excellent still im grateful still able see different stuff hollywood almost flood u saving grace smart enjoyable feel offended marijuana thing better go see america bride sort moviebr br saving grace also show funny movie doesnt stupid laughing as also pondering question female lead character supposed pay deceased husband debtsbr br nutshell witty storyline typical english humour good acting directing couldnt ask morebr br\n"
                }
            ],
            "source": "import random\nidx = random.randint(0, len(df_train))\nprint('------------BEFORE-------------')\nprint(df_train['texts'][idx])\nprint('------------AFTER-------------')\nprint(df_train['preprocessed_texts'][idx])"
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [],
            "source": "df_train_to_save = df_train.reset_index().drop(columns=['texts','index'])\ndf_test_to_save = df_test.reset_index().drop(columns=['texts','index'])"
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>preprocessed_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>barbra streisand first television special must...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>think jason lee huge potential wrong vehicle a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>developed critical eye film love good cinema w...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>pick year event basic instinct catherine trame...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>movie could good come way short cheesy special...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24995</th>\n      <td>0</td>\n      <td>even first minute movie horrific hard believe ...</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>0</td>\n      <td>confessemma opinion single greatest novel ever...</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>0</td>\n      <td>rented horrible movie worst think ever seen be...</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>0</td>\n      <td>wouldnt normally write comment online worst mo...</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>1</td>\n      <td>whether three guy tightywhiteys rapping dude b...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows \u00d7 2 columns</p>\n</div>",
                        "text/plain": "       labels                                 preprocessed_texts\n0           1  barbra streisand first television special must...\n1           0  think jason lee huge potential wrong vehicle a...\n2           1  developed critical eye film love good cinema w...\n3           1  pick year event basic instinct catherine trame...\n4           0  movie could good come way short cheesy special...\n...       ...                                                ...\n24995       0  even first minute movie horrific hard believe ...\n24996       0  confessemma opinion single greatest novel ever...\n24997       0  rented horrible movie worst think ever seen be...\n24998       0  wouldnt normally write comment online worst mo...\n24999       1  whether three guy tightywhiteys rapping dude b...\n\n[25000 rows x 2 columns]"
                    },
                    "execution_count": 70,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_train_to_save"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>preprocessed_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>although movie doesnt darkness book opinion gr...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>jim varneys first real movie quite delight don...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>moment desperation willing whatever take win l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>earlier movie videodrome definitely show simil...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>anymore pretty much reality tv show people mak...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24995</th>\n      <td>0</td>\n      <td>two reason see film first stellan skarsgard ma...</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>0</td>\n      <td>without saying ended sufficient say whole thin...</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>0</td>\n      <td>time ever actually laugh watching show im maki...</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>0</td>\n      <td>found awfully disappointing experience appende...</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>1</td>\n      <td>edgar reitz surprised filmlovers world epic op...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25000 rows \u00d7 2 columns</p>\n</div>",
                        "text/plain": "       labels                                 preprocessed_texts\n0           1  although movie doesnt darkness book opinion gr...\n1           0  jim varneys first real movie quite delight don...\n2           1  moment desperation willing whatever take win l...\n3           1  earlier movie videodrome definitely show simil...\n4           0  anymore pretty much reality tv show people mak...\n...       ...                                                ...\n24995       0  two reason see film first stellan skarsgard ma...\n24996       0  without saying ended sufficient say whole thin...\n24997       0  time ever actually laugh watching show im maki...\n24998       0  found awfully disappointing experience appende...\n24999       1  edgar reitz surprised filmlovers world epic op...\n\n[25000 rows x 2 columns]"
                    },
                    "execution_count": 71,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_test_to_save"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "project.save_data(\"NEW_imdb_train.csv\", df_train_to_save.to_csv(index=False))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "project.save_data(\"NEW_imdb_test.csv\", df_test_to_save.to_csv(index=False))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# EXPERIMENT PART"
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging->ibm-watson-machine-learning) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<1.5,>=0.24.2->autoai-libs==1.14.13) (1.16.0)\nRequirement already satisfied: sortedcontainers~=2.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from portion->jsonsubschema>=0.0.6->lale<0.8,>=0.7) (2.4.0)\nRequirement already satisfied: joblib>=1.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn==1.1.1) (1.1.1)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from xgboost==1.6.2) (1.8.1)\nRequirement already satisfied: joblib>=1.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn->snapml==1.8.12) (2.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mlinsights) (1.16.0)\n"
                }
            ],
            "source": "!pip install ibm-watson-machine-learning | tail -n 1\n!pip install autoai-libs==1.14.13 | tail -n 1\n!pip install 'lale>=0.7,<0.8' | tail -n 1\n!pip install scikit-learn==1.1.1 | tail -n 1\n!pip install xgboost==1.6.2 | tail -n 1\n!pip install lightgbm==3.3.2 | tail -n 1\n!pip install 'snapml==1.8.12' | tail -n 1\n!pip install mlinsights | tail -n 1"
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watson_machine_learning.helpers import DataConnection\nfrom ibm_watson_machine_learning.helpers import ContainerLocation\n\ntraining_data_references = [\n    DataConnection(\n        data_asset_id='315a31f6-653a-4aea-86e3-80d9ea91c5db'\n    ),\n]\n\ntest_data_references = [\n    DataConnection(\n        data_asset_id='d479231c-b273-4b80-b95c-6de26a36a42e'\n    ),\n]\ntraining_result_reference = DataConnection(\n    location=ContainerLocation(\n        path='auto_ml/0f2660ab-21e6-4d9e-b634-0ce09c3c306c/wml_data/b1a74f06-405b-4714-a8fe-8890bc0b905c/data/automl',\n        model_location='auto_ml/0f2660ab-21e6-4d9e-b634-0ce09c3c306c/wml_data/b1a74f06-405b-4714-a8fe-8890bc0b905c/data/automl/model.zip',\n        training_status='auto_ml/0f2660ab-21e6-4d9e-b634-0ce09c3c306c/wml_data/b1a74f06-405b-4714-a8fe-8890bc0b905c/training-status.json'\n    )\n)"
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": "experiment_metadata = dict(\n    prediction_type='binary',\n    prediction_column='labels',\n    holdout_size=0.1,\n    scoring='accuracy',\n    csv_separator=',',\n    random_state=33,\n    max_number_of_estimators=4,\n    training_data_references=training_data_references,\n    training_result_reference=training_result_reference,\n    test_data_references=test_data_references,\n    deployment_url='https://eu-de.ml.cloud.ibm.com',\n    project_id='6a467c5f-dda6-466c-b352-4843920f28e0',\n    positive_label=1,\n    drop_duplicates=True,\n    include_batched_ensemble_estimators=[]\n)"
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": "#watson machine learning service\napi_key = 'X_kVsZIt_N4wyOAXPvkjI0rReJE0q4fzZpDXvMX7i_3R'\nwml_credentials = {\n    \"apikey\": api_key,\n    \"url\": experiment_metadata['deployment_url']\n}"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Get fitted AutoAI optimizer"
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [],
            "source": "from ibm_watson_machine_learning.experiment import AutoAI\n\npipeline_optimizer = AutoAI(wml_credentials, project_id=experiment_metadata['project_id']).runs.get_optimizer(metadata=experiment_metadata)"
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'name': 'New imdb test',\n 'desc': '',\n 'prediction_type': 'classification',\n 'prediction_column': 'labels',\n 'prediction_columns': None,\n 'timestamp_column_name': None,\n 'scoring': 'accuracy',\n 'holdout_size': 0.1,\n 'max_num_daub_ensembles': 4,\n 't_shirt_size': 'a6c4923b-b8e4-444c-9f43-8a7ec3020110',\n 'train_sample_rows_test_size': None,\n 'include_only_estimators': None,\n 'include_batched_ensemble_estimators': None,\n 'backtest_num': None,\n 'lookback_window': None,\n 'forecast_window': None,\n 'backtest_gap_length': None,\n 'cognito_transform_names': None,\n 'data_join_graph': False,\n 'csv_separator': ',',\n 'excel_sheet': None,\n 'encoding': 'utf-8',\n 'positive_label': None,\n 'drop_duplicates': True,\n 'outliers_columns': None,\n 'text_processing': None,\n 'word2vec_feature_number': 30,\n 'daub_give_priority_to_runtime': None,\n 'text_columns_names': None,\n 'sampling_type': None,\n 'sample_size_limit': None,\n 'sample_rows_limit': None,\n 'sample_percentage_limit': None,\n 'number_of_batch_rows': None,\n 'n_parallel_data_connections': None,\n 'test_data_csv_separator': ',',\n 'test_data_excel_sheet': None,\n 'test_data_encoding': 'utf-8',\n 'categorical_imputation_strategy': None,\n 'numerical_imputation_strategy': None,\n 'numerical_imputation_value': None,\n 'imputation_threshold': None,\n 'retrain_on_holdout': None,\n 'feature_columns': None,\n 'pipeline_types': None,\n 'supporting_features_at_forecast': None,\n 'numerical_columns': None,\n 'categorical_columns': None,\n 'confidence_level': None,\n 'incremental_learning': None,\n 'early_stop_enabled': None,\n 'early_stop_window_size': None,\n 'run_id': 'b1a74f06-405b-4714-a8fe-8890bc0b905c'}"
                    },
                    "execution_count": 77,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "pipeline_optimizer.get_params()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Pipeline comparision"
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Enhancements</th>\n      <th>Estimator</th>\n      <th>training_accuracy_(optimized)</th>\n      <th>training_roc_auc</th>\n      <th>holdout_average_precision</th>\n      <th>holdout_log_loss</th>\n      <th>holdout_roc_auc</th>\n      <th>training_balanced_accuracy</th>\n      <th>training_f1</th>\n      <th>holdout_precision</th>\n      <th>training_average_precision</th>\n      <th>training_log_loss</th>\n      <th>holdout_recall</th>\n      <th>training_precision</th>\n      <th>holdout_accuracy</th>\n      <th>holdout_balanced_accuracy</th>\n      <th>training_recall</th>\n      <th>holdout_f1</th>\n    </tr>\n    <tr>\n      <th>Pipeline Name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Pipeline_16</th>\n      <td>HPO, FE, Text_FE, HPO</td>\n      <td>SnapBoostingMachineClassifier</td>\n      <td>0.825677</td>\n      <td>0.902685</td>\n      <td>0.900668</td>\n      <td>0.393948</td>\n      <td>0.904297</td>\n      <td>0.825658</td>\n      <td>0.827935</td>\n      <td>0.824474</td>\n      <td>0.894780</td>\n      <td>0.397598</td>\n      <td>0.821756</td>\n      <td>0.818739</td>\n      <td>0.822849</td>\n      <td>0.822852</td>\n      <td>0.837383</td>\n      <td>0.823113</td>\n    </tr>\n    <tr>\n      <th>Pipeline_14</th>\n      <td>HPO, Text_FE</td>\n      <td>SnapBoostingMachineClassifier</td>\n      <td>0.824352</td>\n      <td>0.900742</td>\n      <td>0.898252</td>\n      <td>0.396315</td>\n      <td>0.903083</td>\n      <td>0.824333</td>\n      <td>0.826552</td>\n      <td>0.827123</td>\n      <td>0.892716</td>\n      <td>0.400883</td>\n      <td>0.817414</td>\n      <td>0.817699</td>\n      <td>0.822728</td>\n      <td>0.822744</td>\n      <td>0.835619</td>\n      <td>0.822240</td>\n    </tr>\n    <tr>\n      <th>Pipeline_15</th>\n      <td>HPO, FE, Text_FE</td>\n      <td>SnapBoostingMachineClassifier</td>\n      <td>0.825074</td>\n      <td>0.901747</td>\n      <td>0.899725</td>\n      <td>0.392963</td>\n      <td>0.903645</td>\n      <td>0.825055</td>\n      <td>0.827392</td>\n      <td>0.823586</td>\n      <td>0.893762</td>\n      <td>0.399662</td>\n      <td>0.819746</td>\n      <td>0.817998</td>\n      <td>0.821518</td>\n      <td>0.821523</td>\n      <td>0.837062</td>\n      <td>0.821662</td>\n    </tr>\n    <tr>\n      <th>Pipeline_13</th>\n      <td>Text_FE</td>\n      <td>SnapBoostingMachineClassifier</td>\n      <td>0.818087</td>\n      <td>0.896059</td>\n      <td>0.892209</td>\n      <td>0.417831</td>\n      <td>0.896871</td>\n      <td>0.818061</td>\n      <td>0.821224</td>\n      <td>0.813395</td>\n      <td>0.889803</td>\n      <td>0.418371</td>\n      <td>0.817254</td>\n      <td>0.808713</td>\n      <td>0.814300</td>\n      <td>0.814290</td>\n      <td>0.834175</td>\n      <td>0.815320</td>\n    </tr>\n    <tr>\n      <th>Pipeline_6</th>\n      <td>HPO, Text_FE</td>\n      <td>ExtraTreesClassifier</td>\n      <td>0.797205</td>\n      <td>0.878246</td>\n      <td>0.870385</td>\n      <td>0.623248</td>\n      <td>0.878199</td>\n      <td>0.797215</td>\n      <td>0.796331</td>\n      <td>0.804277</td>\n      <td>0.868065</td>\n      <td>0.621492</td>\n      <td>0.777054</td>\n      <td>0.801186</td>\n      <td>0.793330</td>\n      <td>0.793381</td>\n      <td>0.791597</td>\n      <td>0.790431</td>\n    </tr>\n    <tr>\n      <th>Pipeline_7</th>\n      <td>HPO, FE, Text_FE</td>\n      <td>ExtraTreesClassifier</td>\n      <td>0.797205</td>\n      <td>0.878246</td>\n      <td>0.870385</td>\n      <td>0.623248</td>\n      <td>0.878199</td>\n      <td>0.797215</td>\n      <td>0.796331</td>\n      <td>0.804277</td>\n      <td>0.868065</td>\n      <td>0.621492</td>\n      <td>0.777054</td>\n      <td>0.801186</td>\n      <td>0.793330</td>\n      <td>0.793381</td>\n      <td>0.791597</td>\n      <td>0.790431</td>\n    </tr>\n    <tr>\n      <th>Pipeline_8</th>\n      <td>HPO, FE, Text_FE, HPO</td>\n      <td>ExtraTreesClassifier</td>\n      <td>0.797205</td>\n      <td>0.878246</td>\n      <td>0.870385</td>\n      <td>0.623248</td>\n      <td>0.878199</td>\n      <td>0.797215</td>\n      <td>0.796331</td>\n      <td>0.804277</td>\n      <td>0.868065</td>\n      <td>0.621492</td>\n      <td>0.777054</td>\n      <td>0.801186</td>\n      <td>0.793330</td>\n      <td>0.793381</td>\n      <td>0.791597</td>\n      <td>0.790431</td>\n    </tr>\n    <tr>\n      <th>Pipeline_2</th>\n      <td>HPO, Text_FE</td>\n      <td>SnapRandomForestClassifier</td>\n      <td>0.790137</td>\n      <td>0.869086</td>\n      <td>0.859257</td>\n      <td>0.579898</td>\n      <td>0.865737</td>\n      <td>0.790118</td>\n      <td>0.792825</td>\n      <td>0.783548</td>\n      <td>0.859634</td>\n      <td>0.576895</td>\n      <td>0.787265</td>\n      <td>0.784000</td>\n      <td>0.784216</td>\n      <td>0.784207</td>\n      <td>0.801860</td>\n      <td>0.785402</td>\n    </tr>\n    <tr>\n      <th>Pipeline_3</th>\n      <td>HPO, FE, Text_FE</td>\n      <td>SnapRandomForestClassifier</td>\n      <td>0.790258</td>\n      <td>0.870532</td>\n      <td>0.860761</td>\n      <td>0.534158</td>\n      <td>0.865928</td>\n      <td>0.790225</td>\n      <td>0.794669</td>\n      <td>0.781342</td>\n      <td>0.863864</td>\n      <td>0.536879</td>\n      <td>0.789194</td>\n      <td>0.779511</td>\n      <td>0.783491</td>\n      <td>0.783473</td>\n      <td>0.810440</td>\n      <td>0.785249</td>\n    </tr>\n    <tr>\n      <th>Pipeline_4</th>\n      <td>HPO, FE, Text_FE, HPO</td>\n      <td>SnapRandomForestClassifier</td>\n      <td>0.790258</td>\n      <td>0.870532</td>\n      <td>0.860761</td>\n      <td>0.534158</td>\n      <td>0.865928</td>\n      <td>0.790225</td>\n      <td>0.794669</td>\n      <td>0.781342</td>\n      <td>0.863864</td>\n      <td>0.536879</td>\n      <td>0.789194</td>\n      <td>0.779511</td>\n      <td>0.783491</td>\n      <td>0.783473</td>\n      <td>0.810440</td>\n      <td>0.785249</td>\n    </tr>\n    <tr>\n      <th>Pipeline_1</th>\n      <td>Text_FE</td>\n      <td>SnapRandomForestClassifier</td>\n      <td>0.782989</td>\n      <td>0.862862</td>\n      <td>0.854874</td>\n      <td>0.483795</td>\n      <td>0.862938</td>\n      <td>0.782967</td>\n      <td>0.786269</td>\n      <td>0.776007</td>\n      <td>0.852304</td>\n      <td>0.486614</td>\n      <td>0.786863</td>\n      <td>0.775791</td>\n      <td>0.779176</td>\n      <td>0.779151</td>\n      <td>0.797049</td>\n      <td>0.781397</td>\n    </tr>\n    <tr>\n      <th>Pipeline_5</th>\n      <td>Text_FE</td>\n      <td>ExtraTreesClassifier</td>\n      <td>0.772950</td>\n      <td>0.853014</td>\n      <td>0.840045</td>\n      <td>0.572063</td>\n      <td>0.850320</td>\n      <td>0.772935</td>\n      <td>0.775236</td>\n      <td>0.775059</td>\n      <td>0.842086</td>\n      <td>0.578420</td>\n      <td>0.761537</td>\n      <td>0.768741</td>\n      <td>0.769538</td>\n      <td>0.769563</td>\n      <td>0.781894</td>\n      <td>0.768239</td>\n    </tr>\n    <tr>\n      <th>Pipeline_11</th>\n      <td>HPO, FE, Text_FE</td>\n      <td>SnapDecisionTreeClassifier</td>\n      <td>0.749297</td>\n      <td>0.821277</td>\n      <td>0.800352</td>\n      <td>0.520390</td>\n      <td>0.819831</td>\n      <td>0.749301</td>\n      <td>0.748813</td>\n      <td>0.756492</td>\n      <td>0.802504</td>\n      <td>0.517761</td>\n      <td>0.744814</td>\n      <td>0.751499</td>\n      <td>0.751754</td>\n      <td>0.751776</td>\n      <td>0.747735</td>\n      <td>0.750608</td>\n    </tr>\n    <tr>\n      <th>Pipeline_12</th>\n      <td>HPO, FE, Text_FE, HPO</td>\n      <td>SnapDecisionTreeClassifier</td>\n      <td>0.751747</td>\n      <td>0.819537</td>\n      <td>0.798512</td>\n      <td>0.522358</td>\n      <td>0.817866</td>\n      <td>0.751717</td>\n      <td>0.756645</td>\n      <td>0.749153</td>\n      <td>0.798317</td>\n      <td>0.521317</td>\n      <td>0.746261</td>\n      <td>0.743172</td>\n      <td>0.747399</td>\n      <td>0.747403</td>\n      <td>0.770668</td>\n      <td>0.747704</td>\n    </tr>\n    <tr>\n      <th>Pipeline_10</th>\n      <td>HPO, Text_FE</td>\n      <td>SnapDecisionTreeClassifier</td>\n      <td>0.727893</td>\n      <td>0.795722</td>\n      <td>0.770419</td>\n      <td>0.549034</td>\n      <td>0.794301</td>\n      <td>0.727855</td>\n      <td>0.734291</td>\n      <td>0.739033</td>\n      <td>0.766179</td>\n      <td>0.547775</td>\n      <td>0.688053</td>\n      <td>0.719172</td>\n      <td>0.721671</td>\n      <td>0.721777</td>\n      <td>0.750862</td>\n      <td>0.712632</td>\n    </tr>\n    <tr>\n      <th>Pipeline_9</th>\n      <td>Text_FE</td>\n      <td>SnapDecisionTreeClassifier</td>\n      <td>0.698699</td>\n      <td>0.698697</td>\n      <td>0.647313</td>\n      <td>10.163411</td>\n      <td>0.705768</td>\n      <td>0.698697</td>\n      <td>0.699439</td>\n      <td>0.710507</td>\n      <td>0.639442</td>\n      <td>10.406701</td>\n      <td>0.697540</td>\n      <td>0.698799</td>\n      <td>0.705742</td>\n      <td>0.705768</td>\n      <td>0.700104</td>\n      <td>0.703964</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                        Enhancements                      Estimator  \\\nPipeline Name                                                         \nPipeline_16    HPO, FE, Text_FE, HPO  SnapBoostingMachineClassifier   \nPipeline_14             HPO, Text_FE  SnapBoostingMachineClassifier   \nPipeline_15         HPO, FE, Text_FE  SnapBoostingMachineClassifier   \nPipeline_13                  Text_FE  SnapBoostingMachineClassifier   \nPipeline_6              HPO, Text_FE           ExtraTreesClassifier   \nPipeline_7          HPO, FE, Text_FE           ExtraTreesClassifier   \nPipeline_8     HPO, FE, Text_FE, HPO           ExtraTreesClassifier   \nPipeline_2              HPO, Text_FE     SnapRandomForestClassifier   \nPipeline_3          HPO, FE, Text_FE     SnapRandomForestClassifier   \nPipeline_4     HPO, FE, Text_FE, HPO     SnapRandomForestClassifier   \nPipeline_1                   Text_FE     SnapRandomForestClassifier   \nPipeline_5                   Text_FE           ExtraTreesClassifier   \nPipeline_11         HPO, FE, Text_FE     SnapDecisionTreeClassifier   \nPipeline_12    HPO, FE, Text_FE, HPO     SnapDecisionTreeClassifier   \nPipeline_10             HPO, Text_FE     SnapDecisionTreeClassifier   \nPipeline_9                   Text_FE     SnapDecisionTreeClassifier   \n\n               training_accuracy_(optimized)  training_roc_auc  \\\nPipeline Name                                                    \nPipeline_16                         0.825677          0.902685   \nPipeline_14                         0.824352          0.900742   \nPipeline_15                         0.825074          0.901747   \nPipeline_13                         0.818087          0.896059   \nPipeline_6                          0.797205          0.878246   \nPipeline_7                          0.797205          0.878246   \nPipeline_8                          0.797205          0.878246   \nPipeline_2                          0.790137          0.869086   \nPipeline_3                          0.790258          0.870532   \nPipeline_4                          0.790258          0.870532   \nPipeline_1                          0.782989          0.862862   \nPipeline_5                          0.772950          0.853014   \nPipeline_11                         0.749297          0.821277   \nPipeline_12                         0.751747          0.819537   \nPipeline_10                         0.727893          0.795722   \nPipeline_9                          0.698699          0.698697   \n\n               holdout_average_precision  holdout_log_loss  holdout_roc_auc  \\\nPipeline Name                                                                 \nPipeline_16                     0.900668          0.393948         0.904297   \nPipeline_14                     0.898252          0.396315         0.903083   \nPipeline_15                     0.899725          0.392963         0.903645   \nPipeline_13                     0.892209          0.417831         0.896871   \nPipeline_6                      0.870385          0.623248         0.878199   \nPipeline_7                      0.870385          0.623248         0.878199   \nPipeline_8                      0.870385          0.623248         0.878199   \nPipeline_2                      0.859257          0.579898         0.865737   \nPipeline_3                      0.860761          0.534158         0.865928   \nPipeline_4                      0.860761          0.534158         0.865928   \nPipeline_1                      0.854874          0.483795         0.862938   \nPipeline_5                      0.840045          0.572063         0.850320   \nPipeline_11                     0.800352          0.520390         0.819831   \nPipeline_12                     0.798512          0.522358         0.817866   \nPipeline_10                     0.770419          0.549034         0.794301   \nPipeline_9                      0.647313         10.163411         0.705768   \n\n               training_balanced_accuracy  training_f1  holdout_precision  \\\nPipeline Name                                                               \nPipeline_16                      0.825658     0.827935           0.824474   \nPipeline_14                      0.824333     0.826552           0.827123   \nPipeline_15                      0.825055     0.827392           0.823586   \nPipeline_13                      0.818061     0.821224           0.813395   \nPipeline_6                       0.797215     0.796331           0.804277   \nPipeline_7                       0.797215     0.796331           0.804277   \nPipeline_8                       0.797215     0.796331           0.804277   \nPipeline_2                       0.790118     0.792825           0.783548   \nPipeline_3                       0.790225     0.794669           0.781342   \nPipeline_4                       0.790225     0.794669           0.781342   \nPipeline_1                       0.782967     0.786269           0.776007   \nPipeline_5                       0.772935     0.775236           0.775059   \nPipeline_11                      0.749301     0.748813           0.756492   \nPipeline_12                      0.751717     0.756645           0.749153   \nPipeline_10                      0.727855     0.734291           0.739033   \nPipeline_9                       0.698697     0.699439           0.710507   \n\n               training_average_precision  training_log_loss  holdout_recall  \\\nPipeline Name                                                                  \nPipeline_16                      0.894780           0.397598        0.821756   \nPipeline_14                      0.892716           0.400883        0.817414   \nPipeline_15                      0.893762           0.399662        0.819746   \nPipeline_13                      0.889803           0.418371        0.817254   \nPipeline_6                       0.868065           0.621492        0.777054   \nPipeline_7                       0.868065           0.621492        0.777054   \nPipeline_8                       0.868065           0.621492        0.777054   \nPipeline_2                       0.859634           0.576895        0.787265   \nPipeline_3                       0.863864           0.536879        0.789194   \nPipeline_4                       0.863864           0.536879        0.789194   \nPipeline_1                       0.852304           0.486614        0.786863   \nPipeline_5                       0.842086           0.578420        0.761537   \nPipeline_11                      0.802504           0.517761        0.744814   \nPipeline_12                      0.798317           0.521317        0.746261   \nPipeline_10                      0.766179           0.547775        0.688053   \nPipeline_9                       0.639442          10.406701        0.697540   \n\n               training_precision  holdout_accuracy  \\\nPipeline Name                                         \nPipeline_16              0.818739          0.822849   \nPipeline_14              0.817699          0.822728   \nPipeline_15              0.817998          0.821518   \nPipeline_13              0.808713          0.814300   \nPipeline_6               0.801186          0.793330   \nPipeline_7               0.801186          0.793330   \nPipeline_8               0.801186          0.793330   \nPipeline_2               0.784000          0.784216   \nPipeline_3               0.779511          0.783491   \nPipeline_4               0.779511          0.783491   \nPipeline_1               0.775791          0.779176   \nPipeline_5               0.768741          0.769538   \nPipeline_11              0.751499          0.751754   \nPipeline_12              0.743172          0.747399   \nPipeline_10              0.719172          0.721671   \nPipeline_9               0.698799          0.705742   \n\n               holdout_balanced_accuracy  training_recall  holdout_f1  \nPipeline Name                                                          \nPipeline_16                     0.822852         0.837383    0.823113  \nPipeline_14                     0.822744         0.835619    0.822240  \nPipeline_15                     0.821523         0.837062    0.821662  \nPipeline_13                     0.814290         0.834175    0.815320  \nPipeline_6                      0.793381         0.791597    0.790431  \nPipeline_7                      0.793381         0.791597    0.790431  \nPipeline_8                      0.793381         0.791597    0.790431  \nPipeline_2                      0.784207         0.801860    0.785402  \nPipeline_3                      0.783473         0.810440    0.785249  \nPipeline_4                      0.783473         0.810440    0.785249  \nPipeline_1                      0.779151         0.797049    0.781397  \nPipeline_5                      0.769563         0.781894    0.768239  \nPipeline_11                     0.751776         0.747735    0.750608  \nPipeline_12                     0.747403         0.770668    0.747704  \nPipeline_10                     0.721777         0.750862    0.712632  \nPipeline_9                      0.705768         0.700104    0.703964  "
                    },
                    "execution_count": 78,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "summary = pipeline_optimizer.summary()\nbest_pipeline_name = list(summary.index)[0]\nsummary"
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [],
            "source": "pipeline_model = pipeline_optimizer.get_pipeline()"
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features_importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>NewFeature_5_sum(NewTextFeature_1_word2vec(preprocessed_texts)__NewTextFeature_2_word2vec(preprocessed_texts))</th>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>NewFeature_11_sum(NewTextFeature_6_word2vec(preprocessed_texts)__NewTextFeature_17_word2vec(preprocessed_texts))</th>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <th>NewFeature_0_sum(NewTextFeature_0_word2vec(preprocessed_texts)__NewTextFeature_12_word2vec(preprocessed_texts))</th>\n      <td>0.62</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_4_word2vec(preprocessed_texts)</th>\n      <td>0.56</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_7_word2vec(preprocessed_texts)</th>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>NewFeature_15_sum(NewTextFeature_15_word2vec(preprocessed_texts)__NewTextFeature_2_word2vec(preprocessed_texts))</th>\n      <td>0.42</td>\n    </tr>\n    <tr>\n      <th>NewFeature_19_sum(log(NewTextFeature_2_word2vec(preprocessed_texts))__NewTextFeature_0_word2vec(preprocessed_texts))</th>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_2_word2vec(preprocessed_texts)</th>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_13_word2vec(preprocessed_texts)</th>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_12_word2vec(preprocessed_texts)</th>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>NewFeature_12_sum(NewTextFeature_9_word2vec(preprocessed_texts)__NewTextFeature_0_word2vec(preprocessed_texts))</th>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>NewFeature_6_sum(NewTextFeature_2_word2vec(preprocessed_texts)__NewTextFeature_10_word2vec(preprocessed_texts))</th>\n      <td>0.07</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_3_word2vec(preprocessed_texts)</th>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_11_word2vec(preprocessed_texts)</th>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_20_word2vec(preprocessed_texts)</th>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_5_word2vec(preprocessed_texts)</th>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>NewFeature_16_sum(NewTextFeature_16_word2vec(preprocessed_texts)__NewTextFeature_0_word2vec(preprocessed_texts))</th>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>NewFeature_10_sum(NewTextFeature_2_word2vec(preprocessed_texts)__NewTextFeature_27_word2vec(preprocessed_texts))</th>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_18_word2vec(preprocessed_texts)</th>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_9_word2vec(preprocessed_texts)</th>\n      <td>0.04</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_6_word2vec(preprocessed_texts)</th>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>NewFeature_17_sum(NewTextFeature_23_word2vec(preprocessed_texts)__NewTextFeature_2_word2vec(preprocessed_texts))</th>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_16_word2vec(preprocessed_texts)</th>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_25_word2vec(preprocessed_texts)</th>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_26_word2vec(preprocessed_texts)</th>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_19_word2vec(preprocessed_texts)</th>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_17_word2vec(preprocessed_texts)</th>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NewFeature_4_sum(NewTextFeature_0_word2vec(preprocessed_texts)__NewTextFeature_29_word2vec(preprocessed_texts))</th>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_14_word2vec(preprocessed_texts)</th>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_24_word2vec(preprocessed_texts)</th>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>NewFeature_7_sum(NewTextFeature_2_word2vec(preprocessed_texts)__NewTextFeature_16_word2vec(preprocessed_texts))</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_29_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_8_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_1_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_27_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_10_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewFeature_14_sum(NewTextFeature_10_word2vec(preprocessed_texts)__NewTextFeature_0_word2vec(preprocessed_texts))</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewFeature_2_sum(NewTextFeature_0_word2vec(preprocessed_texts)__NewTextFeature_26_word2vec(preprocessed_texts))</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_15_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_22_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_28_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_0_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_23_word2vec(preprocessed_texts)</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewFeature_13_sum(NewTextFeature_9_word2vec(preprocessed_texts)__NewTextFeature_2_word2vec(preprocessed_texts))</th>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>NewTextFeature_21_word2vec(preprocessed_texts)</th>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>NewFeature_8_sum(NewTextFeature_2_word2vec(preprocessed_texts)__NewTextFeature_18_word2vec(preprocessed_texts))</th>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>NewFeature_3_sum(NewTextFeature_0_word2vec(preprocessed_texts)__NewTextFeature_27_word2vec(preprocessed_texts))</th>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>NewFeature_9_sum(NewTextFeature_2_word2vec(preprocessed_texts)__NewTextFeature_26_word2vec(preprocessed_texts))</th>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>NewFeature_18_sum(NewTextFeature_27_word2vec(preprocessed_texts)__NewTextFeature_0_word2vec(preprocessed_texts))</th>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>NewFeature_1_sum(NewTextFeature_0_word2vec(preprocessed_texts)__NewTextFeature_14_word2vec(preprocessed_texts))</th>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                                    features_importance\nNewFeature_5_sum(NewTextFeature_1_word2vec(prep...                 1.00\nNewFeature_11_sum(NewTextFeature_6_word2vec(pre...                 0.63\nNewFeature_0_sum(NewTextFeature_0_word2vec(prep...                 0.62\nNewTextFeature_4_word2vec(preprocessed_texts)                      0.56\nNewTextFeature_7_word2vec(preprocessed_texts)                      0.50\nNewFeature_15_sum(NewTextFeature_15_word2vec(pr...                 0.42\nNewFeature_19_sum(log(NewTextFeature_2_word2vec...                 0.25\nNewTextFeature_2_word2vec(preprocessed_texts)                      0.20\nNewTextFeature_13_word2vec(preprocessed_texts)                     0.11\nNewTextFeature_12_word2vec(preprocessed_texts)                     0.08\nNewFeature_12_sum(NewTextFeature_9_word2vec(pre...                 0.07\nNewFeature_6_sum(NewTextFeature_2_word2vec(prep...                 0.07\nNewTextFeature_3_word2vec(preprocessed_texts)                      0.06\nNewTextFeature_11_word2vec(preprocessed_texts)                     0.06\nNewTextFeature_20_word2vec(preprocessed_texts)                     0.06\nNewTextFeature_5_word2vec(preprocessed_texts)                      0.05\nNewFeature_16_sum(NewTextFeature_16_word2vec(pr...                 0.05\nNewFeature_10_sum(NewTextFeature_2_word2vec(pre...                 0.05\nNewTextFeature_18_word2vec(preprocessed_texts)                     0.04\nNewTextFeature_9_word2vec(preprocessed_texts)                      0.04\nNewTextFeature_6_word2vec(preprocessed_texts)                      0.03\nNewFeature_17_sum(NewTextFeature_23_word2vec(pr...                 0.03\nNewTextFeature_16_word2vec(preprocessed_texts)                     0.03\nNewTextFeature_25_word2vec(preprocessed_texts)                     0.03\nNewTextFeature_26_word2vec(preprocessed_texts)                     0.03\nNewTextFeature_19_word2vec(preprocessed_texts)                     0.02\nNewTextFeature_17_word2vec(preprocessed_texts)                     0.02\nNewFeature_4_sum(NewTextFeature_0_word2vec(prep...                 0.02\nNewTextFeature_14_word2vec(preprocessed_texts)                     0.02\nNewTextFeature_24_word2vec(preprocessed_texts)                     0.02\nNewFeature_7_sum(NewTextFeature_2_word2vec(prep...                 0.01\nNewTextFeature_29_word2vec(preprocessed_texts)                     0.01\nNewTextFeature_8_word2vec(preprocessed_texts)                      0.01\nNewTextFeature_1_word2vec(preprocessed_texts)                      0.01\nNewTextFeature_27_word2vec(preprocessed_texts)                     0.01\nNewTextFeature_10_word2vec(preprocessed_texts)                     0.01\nNewFeature_14_sum(NewTextFeature_10_word2vec(pr...                 0.01\nNewFeature_2_sum(NewTextFeature_0_word2vec(prep...                 0.01\nNewTextFeature_15_word2vec(preprocessed_texts)                     0.01\nNewTextFeature_22_word2vec(preprocessed_texts)                     0.01\nNewTextFeature_28_word2vec(preprocessed_texts)                     0.01\nNewTextFeature_0_word2vec(preprocessed_texts)                      0.01\nNewTextFeature_23_word2vec(preprocessed_texts)                     0.01\nNewFeature_13_sum(NewTextFeature_9_word2vec(pre...                 0.01\nNewTextFeature_21_word2vec(preprocessed_texts)                     0.00\nNewFeature_8_sum(NewTextFeature_2_word2vec(prep...                 0.00\nNewFeature_3_sum(NewTextFeature_0_word2vec(prep...                 0.00\nNewFeature_9_sum(NewTextFeature_2_word2vec(prep...                 0.00\nNewFeature_18_sum(NewTextFeature_27_word2vec(pr...                 0.00\nNewFeature_1_sum(NewTextFeature_0_word2vec(prep...                 0.00"
                    },
                    "execution_count": 80,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "pipeline_optimizer.get_pipeline_details()['features_importance']"
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Title: cluster:(root) Pages: 1 -->\n<svg width=\"1271pt\" height=\"82pt\"\n viewBox=\"0.00 0.00 1270.54 81.54\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 77.54)\">\n<title>cluster:(root)</title>\n<g id=\"a_graph0\"><a xlink:title=\"(root) = ...\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-77.54 1266.54,-77.54 1266.54,4 -4,4\"/>\n</a>\n</g>\n<!-- text_transformer -->\n<g id=\"node1\" class=\"node\">\n<title>text_transformer</title>\n<g id=\"a_node1\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.word2vec_transformer.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"text_transformer = TextTransformer(text_processing_options={&#39;word2vec&#39;: {&#39;output_dim&#39;: 30}}, column_headers_list=[&#39;NewTextFeature_0_word2vec(preprocessed_texts)&#39;, &#39;NewTextFeature_1_word2vec(preprocessed_texts)&#39;, &#39;NewTextFeature_2_word2vec(preprocessed_texts)&#39;, &#39;NewTextFeature_3_word2vec...)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"53.03\" cy=\"-36.77\" rx=\"53.07\" ry=\"19.6\"/>\n<text text-anchor=\"middle\" x=\"53.03\" y=\"-39.97\" font-family=\"Times,serif\" font-size=\"11.00\">Text&#45;</text>\n<text text-anchor=\"middle\" x=\"53.03\" y=\"-27.97\" font-family=\"Times,serif\" font-size=\"11.00\">Transformer</text>\n</a>\n</g>\n</g>\n<!-- numpy_column_selector -->\n<g id=\"node2\" class=\"node\">\n<title>numpy_column_selector</title>\n<g id=\"a_node2\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.numpy_column_selector.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"numpy_column_selector = NumpyColumnSelector(columns=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"180.25\" cy=\"-36.77\" rx=\"38.37\" ry=\"28.07\"/>\n<text text-anchor=\"middle\" x=\"180.25\" y=\"-45.97\" font-family=\"Times,serif\" font-size=\"11.00\">Numpy&#45;</text>\n<text text-anchor=\"middle\" x=\"180.25\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">Column&#45;</text>\n<text text-anchor=\"middle\" x=\"180.25\" y=\"-21.97\" font-family=\"Times,serif\" font-size=\"11.00\">Selector</text>\n</a>\n</g>\n</g>\n<!-- text_transformer&#45;&gt;numpy_column_selector -->\n<g id=\"edge1\" class=\"edge\">\n<title>text_transformer&#45;&gt;numpy_column_selector</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106.41,-36.77C114.8,-36.77 123.44,-36.77 131.7,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"131.93,-40.27 141.93,-36.77 131.93,-33.27 131.93,-40.27\"/>\n</g>\n<!-- float_str2_float -->\n<g id=\"node3\" class=\"node\">\n<title>float_str2_float</title>\n<g id=\"a_node3\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.float_str2_float.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"float_str2_float = FloatStr2Float(dtypes_list=[&#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;float_num&#39;, &#39;floa...)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"282.01\" cy=\"-36.77\" rx=\"27.65\" ry=\"28.07\"/>\n<text text-anchor=\"middle\" x=\"282.01\" y=\"-45.97\" font-family=\"Times,serif\" font-size=\"11.00\">Float&#45;</text>\n<text text-anchor=\"middle\" x=\"282.01\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">Str2&#45;</text>\n<text text-anchor=\"middle\" x=\"282.01\" y=\"-21.97\" font-family=\"Times,serif\" font-size=\"11.00\">Float</text>\n</a>\n</g>\n</g>\n<!-- numpy_column_selector&#45;&gt;float_str2_float -->\n<g id=\"edge2\" class=\"edge\">\n<title>numpy_column_selector&#45;&gt;float_str2_float</title>\n<path fill=\"none\" stroke=\"black\" d=\"M218.68,-36.77C227,-36.77 235.81,-36.77 244.11,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.27,-40.27 254.27,-36.77 244.27,-33.27 244.27,-40.27\"/>\n</g>\n<!-- numpy_replace_missing_values -->\n<g id=\"node4\" class=\"node\">\n<title>numpy_replace_missing_values</title>\n<g id=\"a_node4\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.numpy_replace_missing_values.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"numpy_replace_missing_values = NumpyReplaceMissingValues(missing_values=[], filling_values=float(&#39;nan&#39;))\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"385.19\" cy=\"-36.77\" rx=\"39.7\" ry=\"36.54\"/>\n<text text-anchor=\"middle\" x=\"385.19\" y=\"-51.97\" font-family=\"Times,serif\" font-size=\"11.00\">Numpy&#45;</text>\n<text text-anchor=\"middle\" x=\"385.19\" y=\"-39.97\" font-family=\"Times,serif\" font-size=\"11.00\">Replace&#45;</text>\n<text text-anchor=\"middle\" x=\"385.19\" y=\"-27.97\" font-family=\"Times,serif\" font-size=\"11.00\">Missing&#45;</text>\n<text text-anchor=\"middle\" x=\"385.19\" y=\"-15.97\" font-family=\"Times,serif\" font-size=\"11.00\">Values</text>\n</a>\n</g>\n</g>\n<!-- float_str2_float&#45;&gt;numpy_replace_missing_values -->\n<g id=\"edge3\" class=\"edge\">\n<title>float_str2_float&#45;&gt;numpy_replace_missing_values</title>\n<path fill=\"none\" stroke=\"black\" d=\"M309.59,-36.77C317.42,-36.77 326.25,-36.77 335.01,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"335.22,-40.27 345.22,-36.77 335.22,-33.27 335.22,-40.27\"/>\n</g>\n<!-- num_imputer -->\n<g id=\"node5\" class=\"node\">\n<title>num_imputer</title>\n<g id=\"a_node5\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.num_imputer.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"num_imputer = NumImputer(missing_values=float(&#39;nan&#39;), strategy=&#39;median&#39;)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"496.85\" cy=\"-36.77\" rx=\"36.12\" ry=\"19.6\"/>\n<text text-anchor=\"middle\" x=\"496.85\" y=\"-39.97\" font-family=\"Times,serif\" font-size=\"11.00\">Num&#45;</text>\n<text text-anchor=\"middle\" x=\"496.85\" y=\"-27.97\" font-family=\"Times,serif\" font-size=\"11.00\">Imputer</text>\n</a>\n</g>\n</g>\n<!-- numpy_replace_missing_values&#45;&gt;num_imputer -->\n<g id=\"edge4\" class=\"edge\">\n<title>numpy_replace_missing_values&#45;&gt;num_imputer</title>\n<path fill=\"none\" stroke=\"black\" d=\"M424.85,-36.77C433.09,-36.77 441.87,-36.77 450.34,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"450.46,-40.27 460.46,-36.77 450.46,-33.27 450.46,-40.27\"/>\n</g>\n<!-- opt_standard_scaler -->\n<g id=\"node6\" class=\"node\">\n<title>opt_standard_scaler</title>\n<g id=\"a_node6\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.opt_standard_scaler.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"opt_standard_scaler = OptStandardScaler(use_scaler_flag=False)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"612.75\" cy=\"-36.77\" rx=\"43.68\" ry=\"28.07\"/>\n<text text-anchor=\"middle\" x=\"612.75\" y=\"-45.97\" font-family=\"Times,serif\" font-size=\"11.00\">Opt&#45;</text>\n<text text-anchor=\"middle\" x=\"612.75\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">Standard&#45;</text>\n<text text-anchor=\"middle\" x=\"612.75\" y=\"-21.97\" font-family=\"Times,serif\" font-size=\"11.00\">Scaler</text>\n</a>\n</g>\n</g>\n<!-- num_imputer&#45;&gt;opt_standard_scaler -->\n<g id=\"edge5\" class=\"edge\">\n<title>num_imputer&#45;&gt;opt_standard_scaler</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.95,-36.77C541.03,-36.77 549.8,-36.77 558.46,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"558.53,-40.27 568.53,-36.77 558.53,-33.27 558.53,-40.27\"/>\n</g>\n<!-- float32_transform -->\n<g id=\"node7\" class=\"node\">\n<title>float32_transform</title>\n<g id=\"a_node7\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.float32_transform.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"float32_transform = float32_transform()\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"737.14\" cy=\"-36.77\" rx=\"44.6\" ry=\"19.6\"/>\n<text text-anchor=\"middle\" x=\"737.14\" y=\"-39.97\" font-family=\"Times,serif\" font-size=\"11.00\">float32_&#45;</text>\n<text text-anchor=\"middle\" x=\"737.14\" y=\"-27.97\" font-family=\"Times,serif\" font-size=\"11.00\">transform</text>\n</a>\n</g>\n</g>\n<!-- opt_standard_scaler&#45;&gt;float32_transform -->\n<g id=\"edge6\" class=\"edge\">\n<title>opt_standard_scaler&#45;&gt;float32_transform</title>\n<path fill=\"none\" stroke=\"black\" d=\"M656.9,-36.77C665.12,-36.77 673.82,-36.77 682.32,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"682.55,-40.27 692.55,-36.77 682.55,-33.27 682.55,-40.27\"/>\n</g>\n<!-- ta1 -->\n<g id=\"node8\" class=\"node\">\n<title>ta1</title>\n<g id=\"a_node8\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.ta1.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"ta1 = TA1(fun=np.log, name=&#39;log&#39;, datatypes=[&#39;numeric&#39;], feat_constraints=[autoai_libs.utils.fc_methods.is_positive, autoai_libs.utils.fc_methods.is_not_categorical], col_names=[&#39;NewTextFeature_0_word2vec(preprocessed_texts)&#39;, &#39;NewTextFeature_1_word2vec(preproce...)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"844.69\" cy=\"-36.77\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"844.69\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">TA1</text>\n</a>\n</g>\n</g>\n<!-- float32_transform&#45;&gt;ta1 -->\n<g id=\"edge7\" class=\"edge\">\n<title>float32_transform&#45;&gt;ta1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M781.98,-36.77C790.5,-36.77 799.35,-36.77 807.59,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"807.63,-40.27 817.63,-36.77 807.63,-33.27 807.63,-40.27\"/>\n</g>\n<!-- fs1_0 -->\n<g id=\"node9\" class=\"node\">\n<title>fs1_0</title>\n<g id=\"a_node9\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.fs1.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"fs1_0 = FS1(cols_ids_must_keep=range(0, 30), additional_col_count_to_keep=20, ptype=&#39;classification&#39;)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"934.69\" cy=\"-36.77\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"934.69\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">FS1</text>\n</a>\n</g>\n</g>\n<!-- ta1&#45;&gt;fs1_0 -->\n<g id=\"edge8\" class=\"edge\">\n<title>ta1&#45;&gt;fs1_0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M872.09,-36.77C880.08,-36.77 889,-36.77 897.51,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"897.6,-40.27 907.6,-36.77 897.6,-33.27 897.6,-40.27\"/>\n</g>\n<!-- ta2 -->\n<g id=\"node10\" class=\"node\">\n<title>ta2</title>\n<g id=\"a_node10\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.ta2.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"ta2 = TA2(fun=np.add, name=&#39;sum&#39;, datatypes1=[&#39;intc&#39;, &#39;intp&#39;, &#39;int_&#39;, &#39;uint8&#39;, &#39;uint16&#39;, &#39;uint32&#39;, &#39;uint64&#39;, &#39;int8&#39;, &#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;short&#39;, &#39;long&#39;, &#39;longlong&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;], feat_constraints1=[autoai_libs.utils.fc_methods.is_not_...)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"1024.69\" cy=\"-36.77\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1024.69\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">TA2</text>\n</a>\n</g>\n</g>\n<!-- fs1_0&#45;&gt;ta2 -->\n<g id=\"edge9\" class=\"edge\">\n<title>fs1_0&#45;&gt;ta2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M962.09,-36.77C970.08,-36.77 979,-36.77 987.51,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"987.6,-40.27 997.6,-36.77 987.6,-33.27 987.6,-40.27\"/>\n</g>\n<!-- fs1_1 -->\n<g id=\"node11\" class=\"node\">\n<title>fs1_1</title>\n<g id=\"a_node11\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.autoai_libs.fs1.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"fs1_1 = FS1(cols_ids_must_keep=range(0, 30), additional_col_count_to_keep=20, ptype=&#39;classification&#39;)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"1114.69\" cy=\"-36.77\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1114.69\" y=\"-33.97\" font-family=\"Times,serif\" font-size=\"11.00\">FS1</text>\n</a>\n</g>\n</g>\n<!-- ta2&#45;&gt;fs1_1 -->\n<g id=\"edge10\" class=\"edge\">\n<title>ta2&#45;&gt;fs1_1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1052.09,-36.77C1060.08,-36.77 1069,-36.77 1077.51,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1077.6,-40.27 1087.6,-36.77 1077.6,-33.27 1077.6,-40.27\"/>\n</g>\n<!-- snap_boosting_machine_classifier -->\n<g id=\"node12\" class=\"node\">\n<title>snap_boosting_machine_classifier</title>\n<g id=\"a_node12\"><a xlink:href=\"https://lale.readthedocs.io/en/latest/modules/lale.lib.snapml.snap_boosting_machine_classifier.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer\" xlink:title=\"snap_boosting_machine_classifier = SnapBoostingMachineClassifier(class_weight=&#39;balanced&#39;, learning_rate=0.2995596246735124, max_max_depth=3, min_max_depth=3, num_round=81, random_state=33)\">\n<ellipse fill=\"white\" stroke=\"black\" cx=\"1220.11\" cy=\"-36.77\" rx=\"42.35\" ry=\"36.54\"/>\n<text text-anchor=\"middle\" x=\"1220.11\" y=\"-51.97\" font-family=\"Times,serif\" font-size=\"11.00\">Snap&#45;</text>\n<text text-anchor=\"middle\" x=\"1220.11\" y=\"-39.97\" font-family=\"Times,serif\" font-size=\"11.00\">Boosting&#45;</text>\n<text text-anchor=\"middle\" x=\"1220.11\" y=\"-27.97\" font-family=\"Times,serif\" font-size=\"11.00\">Machine&#45;</text>\n<text text-anchor=\"middle\" x=\"1220.11\" y=\"-15.97\" font-family=\"Times,serif\" font-size=\"11.00\">Classifier</text>\n</a>\n</g>\n</g>\n<!-- fs1_1&#45;&gt;snap_boosting_machine_classifier -->\n<g id=\"edge11\" class=\"edge\">\n<title>fs1_1&#45;&gt;snap_boosting_machine_classifier</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1141.78,-36.77C1149.53,-36.77 1158.29,-36.77 1167.07,-36.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1167.32,-40.27 1177.32,-36.77 1167.32,-33.27 1167.32,-40.27\"/>\n</g>\n</g>\n</svg>\n",
                        "text/plain": "<graphviz.graphs.Digraph at 0x7f697c923a60>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": "pipeline_model.visualize()"
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;texttransformer&#x27;,\n                 TextTransformer(column_headers_list=[&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_5_word2vec(preprocesse...\n                 autoai_libs.cognito.transforms.transform_utils.TA2(fun = numpy.add, name = &#x27;sum&#x27;, datatypes1 = [&#x27;intc&#x27;, &#x27;intp&#x27;, &#x27;int_&#x27;, &#x27;uint8&#x27;, &#x27;uint16&#x27;, &#x27;uint32&#x27;, &#x27;uint64&#x27;, &#x27;int8&#x27;, &#x27;int16&#x27;, &#x27;int32&#x27;, &#x27;int64&#x27;, &#x27;short&#x27;, &#x27;long&#x27;, &#x27;longlong&#x27;, &#x27;float16&#x27;, &#x27;float32&#x27;, &#x27;float64&#x27;], feat_constraints1 = [&lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], datatypes2 = [&#x27;intc&#x27;, &#x27;intp&#x27;, &#x27;int_&#x27;, &#x27;uint8&#x27;, &#x27;uint16&#x27;, &#x27;uint32&#x27;, &#x27;uint64&#x27;, &#x27;int8&#x27;, &#x27;int16&#x27;, &#x27;int32&#x27;, &#x27;int64&#x27;, &#x27;short&#x27;, &#x27;long&#x27;, &#x27;longlong&#x27;, &#x27;float16&#x27;, &#x27;float32&#x27;, &#x27;float64&#x27;], feat_constraints2 = [&lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], tgraph = None, apply_all = True, col_names = [&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_5_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_6_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_7_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_8_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_9_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_10_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_11_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_12_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_13_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_14_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_15_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_16_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_17_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_18_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_19_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_20_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_21_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_22_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_23_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_24_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_25_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_26_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_27_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_28_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_29_word2vec(preprocessed_texts)&#x27;, &#x27;log(NewTextFeature_0_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_1_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_2_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_3_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_4_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_5_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_6_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_7_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_9_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_11_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_12_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_13_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_15_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_16_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_17_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_19_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_20_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_22_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_24_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_26_word2vec(preprocessed_texts))&#x27;], col_dtypes = [dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;)], col_as_json_objects = None)),\n                (&#x27;fs1-2&#x27;,\n                 autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 30), additional_col_count_to_keep = 20, ptype = &#x27;classification&#x27;)),\n                (&#x27;snapboostingmachineclassifier&#x27;,\n                 SnapBoostingMachineClassifier(class_weight=&#x27;balanced&#x27;,\n                                               learning_rate=0.2995596246735124,\n                                               max_max_depth=3, min_max_depth=3,\n                                               num_round=81,\n                                               random_state=33))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;texttransformer&#x27;,\n                 TextTransformer(column_headers_list=[&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;,\n                                                      &#x27;NewTextFeature_5_word2vec(preprocesse...\n                 autoai_libs.cognito.transforms.transform_utils.TA2(fun = numpy.add, name = &#x27;sum&#x27;, datatypes1 = [&#x27;intc&#x27;, &#x27;intp&#x27;, &#x27;int_&#x27;, &#x27;uint8&#x27;, &#x27;uint16&#x27;, &#x27;uint32&#x27;, &#x27;uint64&#x27;, &#x27;int8&#x27;, &#x27;int16&#x27;, &#x27;int32&#x27;, &#x27;int64&#x27;, &#x27;short&#x27;, &#x27;long&#x27;, &#x27;longlong&#x27;, &#x27;float16&#x27;, &#x27;float32&#x27;, &#x27;float64&#x27;], feat_constraints1 = [&lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], datatypes2 = [&#x27;intc&#x27;, &#x27;intp&#x27;, &#x27;int_&#x27;, &#x27;uint8&#x27;, &#x27;uint16&#x27;, &#x27;uint32&#x27;, &#x27;uint64&#x27;, &#x27;int8&#x27;, &#x27;int16&#x27;, &#x27;int32&#x27;, &#x27;int64&#x27;, &#x27;short&#x27;, &#x27;long&#x27;, &#x27;longlong&#x27;, &#x27;float16&#x27;, &#x27;float32&#x27;, &#x27;float64&#x27;], feat_constraints2 = [&lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], tgraph = None, apply_all = True, col_names = [&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_5_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_6_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_7_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_8_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_9_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_10_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_11_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_12_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_13_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_14_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_15_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_16_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_17_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_18_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_19_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_20_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_21_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_22_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_23_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_24_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_25_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_26_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_27_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_28_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_29_word2vec(preprocessed_texts)&#x27;, &#x27;log(NewTextFeature_0_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_1_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_2_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_3_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_4_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_5_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_6_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_7_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_9_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_11_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_12_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_13_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_15_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_16_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_17_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_19_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_20_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_22_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_24_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_26_word2vec(preprocessed_texts))&#x27;], col_dtypes = [dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;)], col_as_json_objects = None)),\n                (&#x27;fs1-2&#x27;,\n                 autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 30), additional_col_count_to_keep = 20, ptype = &#x27;classification&#x27;)),\n                (&#x27;snapboostingmachineclassifier&#x27;,\n                 SnapBoostingMachineClassifier(class_weight=&#x27;balanced&#x27;,\n                                               learning_rate=0.2995596246735124,\n                                               max_max_depth=3, min_max_depth=3,\n                                               num_round=81,\n                                               random_state=33))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TextTransformer</label><div class=\"sk-toggleable__content\"><pre>TextTransformer(column_headers_list=[&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_5_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_6_word2ve...\n                                     &#x27;NewTextFeature_25_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_26_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_27_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_28_word2vec(preprocessed_texts)&#x27;,\n                                     &#x27;NewTextFeature_29_word2vec(preprocessed_texts)&#x27;],\n                columns_to_be_deleted=[0], drop_columns=True, text_columns=[0],\n                text_processing_options={&#x27;word2vec&#x27;: {&#x27;output_dim&#x27;: 30}})</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumpyColumnSelector</label><div class=\"sk-toggleable__content\"><pre>NumpyColumnSelector(columns=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                             15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n                             28, 29])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FloatStr2Float</label><div class=\"sk-toggleable__content\"><pre>FloatStr2Float(dtypes_list=[&#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;, &#x27;float_num&#x27;,\n                            &#x27;float_num&#x27;, &#x27;float_num&#x27;],\n               missing_values_reference_list=[])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumpyReplaceMissingValues</label><div class=\"sk-toggleable__content\"><pre>NumpyReplaceMissingValues(missing_values=[])</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumImputer</label><div class=\"sk-toggleable__content\"><pre>NumImputer(missing_values=nan, strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OptStandardScaler</label><div class=\"sk-toggleable__content\"><pre>OptStandardScaler(use_scaler_flag=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">float32_transform</label><div class=\"sk-toggleable__content\"><pre>float32_transform()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TA1</label><div class=\"sk-toggleable__content\"><pre>autoai_libs.cognito.transforms.transform_utils.TA1(fun = numpy.log, name = &#x27;log&#x27;, datatypes = [&#x27;numeric&#x27;], feat_constraints = [&lt;cyfunction is_positive at 0x7f69623abe00&gt;, &lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], tgraph = None, apply_all = True, col_names = [&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_5_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_6_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_7_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_8_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_9_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_10_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_11_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_12_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_13_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_14_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_15_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_16_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_17_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_18_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_19_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_20_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_21_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_22_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_23_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_24_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_25_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_26_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_27_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_28_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_29_word2vec(preprocessed_texts)&#x27;], col_dtypes = [dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;)], col_as_json_objects = None)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FS1</label><div class=\"sk-toggleable__content\"><pre>autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 30), additional_col_count_to_keep = 20, ptype = &#x27;classification&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TA2</label><div class=\"sk-toggleable__content\"><pre>autoai_libs.cognito.transforms.transform_utils.TA2(fun = numpy.add, name = &#x27;sum&#x27;, datatypes1 = [&#x27;intc&#x27;, &#x27;intp&#x27;, &#x27;int_&#x27;, &#x27;uint8&#x27;, &#x27;uint16&#x27;, &#x27;uint32&#x27;, &#x27;uint64&#x27;, &#x27;int8&#x27;, &#x27;int16&#x27;, &#x27;int32&#x27;, &#x27;int64&#x27;, &#x27;short&#x27;, &#x27;long&#x27;, &#x27;longlong&#x27;, &#x27;float16&#x27;, &#x27;float32&#x27;, &#x27;float64&#x27;], feat_constraints1 = [&lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], datatypes2 = [&#x27;intc&#x27;, &#x27;intp&#x27;, &#x27;int_&#x27;, &#x27;uint8&#x27;, &#x27;uint16&#x27;, &#x27;uint32&#x27;, &#x27;uint64&#x27;, &#x27;int8&#x27;, &#x27;int16&#x27;, &#x27;int32&#x27;, &#x27;int64&#x27;, &#x27;short&#x27;, &#x27;long&#x27;, &#x27;longlong&#x27;, &#x27;float16&#x27;, &#x27;float32&#x27;, &#x27;float64&#x27;], feat_constraints2 = [&lt;cyfunction is_not_categorical at 0x7f6962208450&gt;], tgraph = None, apply_all = True, col_names = [&#x27;NewTextFeature_0_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_1_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_2_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_3_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_4_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_5_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_6_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_7_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_8_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_9_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_10_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_11_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_12_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_13_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_14_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_15_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_16_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_17_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_18_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_19_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_20_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_21_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_22_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_23_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_24_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_25_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_26_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_27_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_28_word2vec(preprocessed_texts)&#x27;, &#x27;NewTextFeature_29_word2vec(preprocessed_texts)&#x27;, &#x27;log(NewTextFeature_0_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_1_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_2_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_3_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_4_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_5_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_6_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_7_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_9_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_11_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_12_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_13_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_15_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_16_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_17_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_19_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_20_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_22_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_24_word2vec(preprocessed_texts))&#x27;, &#x27;log(NewTextFeature_26_word2vec(preprocessed_texts))&#x27;], col_dtypes = [dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;), dtype(&#x27;float32&#x27;)], col_as_json_objects = None)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FS1</label><div class=\"sk-toggleable__content\"><pre>autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 30), additional_col_count_to_keep = 20, ptype = &#x27;classification&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SnapBoostingMachineClassifier</label><div class=\"sk-toggleable__content\"><pre>SnapBoostingMachineClassifier(class_weight=&#x27;balanced&#x27;,\n                              learning_rate=0.2995596246735124, max_max_depth=3,\n                              min_max_depth=3, num_round=81, random_state=33)</pre></div></div></div></div></div></div></div>",
                        "text/plain": "Pipeline(steps=[('texttransformer',\n                 TextTransformer(column_headers_list=['NewTextFeature_0_word2vec(preprocessed_texts)',\n                                                      'NewTextFeature_1_word2vec(preprocessed_texts)',\n                                                      'NewTextFeature_2_word2vec(preprocessed_texts)',\n                                                      'NewTextFeature_3_word2vec(preprocessed_texts)',\n                                                      'NewTextFeature_4_word2vec(preprocessed_texts)',\n                                                      'NewTextFeature_5_word2vec(preprocesse...\n                 autoai_libs.cognito.transforms.transform_utils.TA2(fun = numpy.add, name = 'sum', datatypes1 = ['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints1 = [<cyfunction is_not_categorical at 0x7f6962208450>], datatypes2 = ['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints2 = [<cyfunction is_not_categorical at 0x7f6962208450>], tgraph = None, apply_all = True, col_names = ['NewTextFeature_0_word2vec(preprocessed_texts)', 'NewTextFeature_1_word2vec(preprocessed_texts)', 'NewTextFeature_2_word2vec(preprocessed_texts)', 'NewTextFeature_3_word2vec(preprocessed_texts)', 'NewTextFeature_4_word2vec(preprocessed_texts)', 'NewTextFeature_5_word2vec(preprocessed_texts)', 'NewTextFeature_6_word2vec(preprocessed_texts)', 'NewTextFeature_7_word2vec(preprocessed_texts)', 'NewTextFeature_8_word2vec(preprocessed_texts)', 'NewTextFeature_9_word2vec(preprocessed_texts)', 'NewTextFeature_10_word2vec(preprocessed_texts)', 'NewTextFeature_11_word2vec(preprocessed_texts)', 'NewTextFeature_12_word2vec(preprocessed_texts)', 'NewTextFeature_13_word2vec(preprocessed_texts)', 'NewTextFeature_14_word2vec(preprocessed_texts)', 'NewTextFeature_15_word2vec(preprocessed_texts)', 'NewTextFeature_16_word2vec(preprocessed_texts)', 'NewTextFeature_17_word2vec(preprocessed_texts)', 'NewTextFeature_18_word2vec(preprocessed_texts)', 'NewTextFeature_19_word2vec(preprocessed_texts)', 'NewTextFeature_20_word2vec(preprocessed_texts)', 'NewTextFeature_21_word2vec(preprocessed_texts)', 'NewTextFeature_22_word2vec(preprocessed_texts)', 'NewTextFeature_23_word2vec(preprocessed_texts)', 'NewTextFeature_24_word2vec(preprocessed_texts)', 'NewTextFeature_25_word2vec(preprocessed_texts)', 'NewTextFeature_26_word2vec(preprocessed_texts)', 'NewTextFeature_27_word2vec(preprocessed_texts)', 'NewTextFeature_28_word2vec(preprocessed_texts)', 'NewTextFeature_29_word2vec(preprocessed_texts)', 'log(NewTextFeature_0_word2vec(preprocessed_texts))', 'log(NewTextFeature_1_word2vec(preprocessed_texts))', 'log(NewTextFeature_2_word2vec(preprocessed_texts))', 'log(NewTextFeature_3_word2vec(preprocessed_texts))', 'log(NewTextFeature_4_word2vec(preprocessed_texts))', 'log(NewTextFeature_5_word2vec(preprocessed_texts))', 'log(NewTextFeature_6_word2vec(preprocessed_texts))', 'log(NewTextFeature_7_word2vec(preprocessed_texts))', 'log(NewTextFeature_9_word2vec(preprocessed_texts))', 'log(NewTextFeature_11_word2vec(preprocessed_texts))', 'log(NewTextFeature_12_word2vec(preprocessed_texts))', 'log(NewTextFeature_13_word2vec(preprocessed_texts))', 'log(NewTextFeature_15_word2vec(preprocessed_texts))', 'log(NewTextFeature_16_word2vec(preprocessed_texts))', 'log(NewTextFeature_17_word2vec(preprocessed_texts))', 'log(NewTextFeature_19_word2vec(preprocessed_texts))', 'log(NewTextFeature_20_word2vec(preprocessed_texts))', 'log(NewTextFeature_22_word2vec(preprocessed_texts))', 'log(NewTextFeature_24_word2vec(preprocessed_texts))', 'log(NewTextFeature_26_word2vec(preprocessed_texts))'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')], col_as_json_objects = None)),\n                ('fs1-2',\n                 autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 30), additional_col_count_to_keep = 20, ptype = 'classification')),\n                ('snapboostingmachineclassifier',\n                 SnapBoostingMachineClassifier(class_weight='balanced',\n                                               learning_rate=0.2995596246735124,\n                                               max_max_depth=3, min_max_depth=3,\n                                               num_round=81,\n                                               random_state=33))])"
                    },
                    "execution_count": 82,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn import set_config\nset_config(display=\"diagram\")\nscikit_learn_pipeline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Some predictions"
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_texts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I watch and re-watch a lot of movies per year ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This film is quite stupid and illogical. I am ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This is one of the weirdest movies I've ever w...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Have to love Michelle Yeon in this film, she w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What a smooth synergy of sci-fi, action, comed...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                  preprocessed_texts\n0  I watch and re-watch a lot of movies per year ...\n1  This film is quite stupid and illogical. I am ...\n2  This is one of the weirdest movies I've ever w...\n3  Have to love Michelle Yeon in this film, she w...\n4  What a smooth synergy of sci-fi, action, comed..."
                    },
                    "execution_count": 83,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "No NaNs\n"
                }
            ],
            "source": "clean_input = Preprocessing().check_nans(data['preprocessed_texts'])\ntransformed_input= Preprocessing().transform(clean_input)\ndata['for_prediction'] = transformed_input"
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_texts</th>\n      <th>for_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I watch and re-watch a lot of movies per year ...</td>\n      <td>watch rewatch lot movie per year hobby money i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This film is quite stupid and illogical. I am ...</td>\n      <td>film quite stupid illogical pretty sure people...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This is one of the weirdest movies I've ever w...</td>\n      <td>one weirdest movie ive ever watched somewhat e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Have to love Michelle Yeon in this film, she w...</td>\n      <td>love michelle yeon film great usual rest well ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What a smooth synergy of sci-fi, action, comed...</td>\n      <td>smooth synergy scifi action comedy best editin...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                                  preprocessed_texts  \\\n0  I watch and re-watch a lot of movies per year ...   \n1  This film is quite stupid and illogical. I am ...   \n2  This is one of the weirdest movies I've ever w...   \n3  Have to love Michelle Yeon in this film, she w...   \n4  What a smooth synergy of sci-fi, action, comed...   \n\n                                      for_prediction  \n0  watch rewatch lot movie per year hobby money i...  \n1  film quite stupid illogical pretty sure people...  \n2  one weirdest movie ive ever watched somewhat e...  \n3  love michelle yeon film great usual rest well ...  \n4  smooth synergy scifi action comedy best editin...  "
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#see the data\ndata"
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [],
            "source": "input_list = list()\nfor row in data['for_prediction']:\n    input_list.append([row])"
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Scoring response\n{'predictions': [{'fields': ['prediction', 'probability'], 'values': [[0, [0.9166188807842924, 0.08338111921570754]], [0, [0.7841306327013434, 0.2158693672986566]], [0, [0.7048409619646661, 0.2951590380353339]], [1, [0.08740278386172307, 0.9125972161382769]], [1, [0.1830348515978495, 0.8169651484021505]]]}]}\n"
                }
            ],
            "source": "import requests\n\n# NOTE: you must manually set API_KEY below using information retrieved from your IBM Cloud account.\nAPI_KEY = \"X_kVsZIt_N4wyOAXPvkjI0rReJE0q4fzZpDXvMX7i_3R\"\ntoken_response = requests.post('https://iam.cloud.ibm.com/identity/token', data={\"apikey\":\n API_KEY, \"grant_type\": 'urn:ibm:params:oauth:grant-type:apikey'})\nmltoken = token_response.json()[\"access_token\"]\n\nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\n\n# NOTE: manually define and pass the array(s) of values to be scored in the next line\npayload_scoring = {\"input_data\": [{\"fields\": ['preprocessed_texts'], \"values\": input_list}]}\n\nresponse_scoring = requests.post('https://eu-de.ml.cloud.ibm.com/ml/v4/deployments/imdb_model_final/predictions?version=2023-05-09', json=payload_scoring,\n headers={'Authorization': 'Bearer ' + mltoken})\nprint(\"Scoring response\")\nprint(response_scoring.json())"
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "----------- REVIEW TEXT -----------\nI watch and re-watch a lot of movies per year as a hobby (no money involved), and today I have 9988 reviews in IMDb. Last month, I received an email from IMDb listing \"Everything Everywhere All at Once\" as one of the Top-10 movies of 2022. In IMDb, it is informed that this flick is nominated for 10 BAFTA Awards, 240 wins and 351 nominations. I can only understand that this is a heard behavior to the promotion of studio, using \"professional critics\" and press to promote such a garbage. I cannot envision a normal being, without financial interest or being manipulated by critics, to enjoy this crap. It seems to be a bad trip of the writers turned into a movie by insane productors. In the end, this film is 2h 19 min of complete waste of time. My vote is one (awful).\n----------------------\nModel prediction: negative\nWith confidence 0.9166188807842924\n----------- REVIEW TEXT -----------\nThis film is quite stupid and illogical. I am pretty sure that most people are too afraid to admit they don't understand it. It's like with \"The Emperor's New Clothes\". Everyone can see the Emperor has no clothes on, but by revealing their knowledge they are afraid that everyone else will believe they are stupid. I am the little boy spontaneously shouting that the Emperor is naked. Also, please notice that I watch the last half an hour of the film at double speed. That's how boring the film is. Finally, I do wonder how many of the jury's members actually watched this movie?! That said, this isn't the first time some weird inconceivable movie has won an Oscar.\n----------------------\nModel prediction: negative\nWith confidence 0.7841306327013434\n----------- REVIEW TEXT -----------\nThis is one of the weirdest movies I've ever watched. It was somewhat enjoyable, yet extremely disturbing, confusing and unbelievably weird and many thing made me go what the bloody f... is happening. There's certainly a message,but omg, it seems everyone in this movie was under some weird substances to produce this movie.\n----------------------\nModel prediction: negative\nWith confidence 0.7048409619646661\n----------- REVIEW TEXT -----------\nHave to love Michelle Yeon in this film, she was great, as usual.  The rest of it? Well, on occasion amusing, but mostly a waste of time This parallel universe smorgasbord pretends truly Deep Thoughts, but essentially boils down to a young lady having a temper tantrum, convinced her mother doesn't understand her. Oh, really? Yawn. The ultimate answer? Love your family. Uh huh.  Yeah, those \"insights\" have been stated many, many times before, usually in a much more elegant and impactful way than does this film. The movie has a few good moments, but falls pitifully short of its grand aspirations.\n----------------------\nModel prediction: positive\nWith confidence 0.9125972161382769\n----------- REVIEW TEXT -----------\nWhat a smooth synergy of sci-fi, action, comedy and some of the best editing I've seen in a long time. The psychological and story-telling elements are reminiscent of Terry Gilliam, but with a much bigger budget, and over-the-top comedic action reminiscent of Jackie Chan and Deadpool.  But to me it's the Oscar-worthy editing that really shines, so shiny that I'm dazzled enough to overlook what I'd consider the movies flaws: namely some pacing issues and the inability to give the film some real heart. (Let's be honest: sexual-orientation issues are passe at this juncture, and just come off as low-hanging fruit.) But the whole experience seeing this in a theater filled with riotous laughter was truly memorable.  My honest wish: that the fake ending was the real ending, followed by a \"To Be Continued.\" I would have genuinely preferred this film to have been a two- or three-parter.\n----------------------\nModel prediction: positive\nWith confidence 0.8169651484021505\n"
                }
            ],
            "source": "cat = ['negative', 'positive']\nfor idx_rev in range(len(data)):\n    print('----------- REVIEW TEXT -----------')\n    print(data['preprocessed_texts'][idx_rev])\n    print('----------------------')\n    model_pred = response_scoring.json()['predictions'][0]['values'][idx_rev][0]\n    print(f'Model prediction: {cat[model_pred]}')\n    conf = response_scoring.json()['predictions'][0]['values'][idx_rev][1][model_pred]\n    print(f'With confidence {conf}')"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}